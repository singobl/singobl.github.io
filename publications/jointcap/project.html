<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="Content-Language" content="zh-cn" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Learning Efficient Illumination Multiplexing for Joint Capture of Reflectance and Shape</title>
<link href="../../gly.css" rel="stylesheet" type="text/css">

<style type="text/css">
.style3 {
				font-family: Georgia, Times, serif;
				font-size: 16pt;
				font-style: normal;
				color: #000000;
}
.style7 {
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				color: #000000;
}
.style8 {
				text-align: center;
}
.style9 {
				/* fixed width */
	margin: 0 auto;
				padding: 30px;
				width: 720px;
				text-align: center;
				position: relative;
				font-family: Georgia, Times, serif;
				font-size: 10pt;
				font-style: normal;
				font-weight: normal;
				color: rgb(00,00,00);/*#000000;*/;
				background-color: rgb(255,255,255);
				-moz-border-radius: 15px;
				border-radius: 15px;
				-moz-box-shadow: 4px 4px 6px #888;
				-webkit-box-shadow: 4px 4px 6px #888;
				box-shadow: 4px 4px 6px #888;
}
.style10 {
				text-align: left;
}
.style11 {
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				font-weight: normal;
				color: #000000;
}
.style12 {
				font-family: Georgia, Times, serif;
				font-size: 10pt;
				font-style: normal;
				color: #000000;
}
.style13 {
				text-align: center;
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				color: #000000;
}
.style14 {
				font-family: Georgia, Times, serif;
				font-style: normal;
				font-weight: bold;
				color: #000000;
}
</style>

</head>

<body>
<div class="style9">
<div class="style8">
<span class="style3">Learning Efficient Illumination Multiplexing for<br />
 Joint Capture of Reflectance and Shape<br />
</span>
<br />
<span class="style7">
<a href="http://www.cocoakang.cn">Kaizhang Kang</a>, 
	  Cihui Xie, 
	  <a href="http://cs.yale.edu/homes/che/">Chengan He</a>,
	  Mingqi Yi,
	  <br />
	  Minyi Gu,
	  Zimin Chen, 
	  <a href="http://kunzhou.net/">Kun Zhou</a> and
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>
</span><br />
<span class="text"><br />
</span><span class="style11">ACM Trans. Graph. (Proc. SIGGRAPH Asia 2019), 38, 6 (Nov. 2019), 165.<br />
Patent Pending.</span>
<br />
<em><br />
</em>
<br />
				<img alt="" src="teaser.jpg" width="700" class="papericon" /><br /><br />
				<img src="jointcap01.jpg" width="114" class="papericon">
				<img src="jointcap02.jpg" width="114" class="papericon">
				<img src="jointcap03.jpg" width="114" class="papericon">
				<img src="jointcap04.jpg" width="114" class="papericon">
				<img src="jointcap05.jpg" width="114" class="papericon">
				<img src="jointcap06.jpg" width="114" class="papericon">
				<img src="jointcap07.jpg" width="114" class="papericon">
				<img src="jointcap08.jpg" width="114" class="papericon">
				<img src="jointcap09.jpg" width="114" class="papericon">
				<img src="jointcap10.jpg" width="114" class="papericon">
				<img src="jointcap11.jpg" width="114" class="papericon">
				<img src="jointcap12.jpg" width="114" class="papericon">
				<br />
				<br />
</div>
<hr />
</span>
<div class="style10">
				<span class="textsectionheader2"><br />
				Abstract</span><span class="text"><br />
				<br />
				We propose a novel framework that automatically learns the lighting patterns for efficient, joint acquisition of unknown reflectance and shape. The core of our framework is a deep neural network, with a shared linear encoder that directly corresponds to the lighting patterns used in physical acquisition, as well as non-linear decoders that output per-pixel normal and diffuse / specular information from photographs. We exploit the diffuse and normal information from multiple views to reconstruct a detailed 3D shape, and then fit BRDF parameters to the diffuse / specular information, producing texture maps as reflectance results. We demonstrate the effectiveness of the framework with physical objects that vary considerably in reflectance and shape, acquired with as few as 16~32 lighting patterns that correspond to 7~15 seconds of per-view acquisition time. Our framework is useful for optimizing the efficiency in both novel and existing setups, as it can automatically adapt to various factors, including the geometry / the lighting layout of the device and the properties of appearance.<br />
				</span><br />
				<hr /><br />
				<span class="textsectionheader2">Downloads
				<br />
				</span>
				<span class="style14">
				<br />
				</span><span class="style12">Paper <a href="jointcap.pdf">[.PDF, Low-res, 6.7MB]</a> <a href="https://doi.org/10.1145/3355089.3356492">[ACM Digital Library]</a></span><br />
				<br />
				Bibtex <a href="jointcap.bib">[.BIB]</a><br />
				<br />
				Video
				<a href="jointcap.mp4">[.MP4, 99.8MB]</a>
				<a href="https://youtu.be/Dll88pBNKgc" target=" _blank">[Youtube]</a>
				<br />
				<br />
				Slides <a href="jointcap_ppt_lq.pdf">[.PDF, 2.1MB]</a><br />
				<br />
				</span> 
				<hr />				
<div class="style10">
				<span class="textsectionheader2"><br />
				Code & Data</span><span class="text"><br />
				<br />
				Our source code is released under the <a href="https://www.gnu.org/licenses/gpl-3.0.en.html">GPLv3 license</a> for acadmic purposes. The only requirement for using the code in your research is to <a href="jointcap.bib">cite our paper[.BIB]</a>. For commercial licensing options, please email hwu at acm.org. For technical issues, please email cocoa_kang at zju.edu.cn.
				<br />
				<br />
The link to our source repository is <a href="https://github.com/cocoakang/thinking-lightstage">https://github.com/cocoakang/thinking-lightstage</a>.
				<br />
				<br />
It contains the scripts for training the neural network proposed in our paper, as well as a module for synthetic training data generation, using the TensorFlow framework. After downloading the repository, please run siga19_source\train.bat (Windows) or train.sh (Linux) to train the network with synthetic lumitexels generated on-the-fly. For more details, please refer to the source code along with its README documents. The current version does not include any test cases yet. Please check the online repository later as we are updating.
				<br />
				<br />
				</span> 
				<hr />				
<br />
<div class="style13">
<a href="../../index.html">Back</a></div>
</div>
</body>

</html>
