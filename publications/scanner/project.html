<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="Content-Language" content="zh-cn" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Free-form Scanning of Non-planar Appearance with Neural Trace Photography</title>
<link href="../../gly.css" rel="stylesheet" type="text/css">

<style type="text/css">
.style3 {
				font-family: Georgia, Times, serif;
				font-size: 16pt;
				font-style: normal;
				color: #000000;
}
.style7 {
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				color: #000000;
}
.style8 {
				text-align: center;
}
.style9 {
				/* fixed width */
	margin: 0 auto;
				padding: 30px;
				width: 720px;
				text-align: center;
				position: relative;
				font-family: Georgia, Times, serif;
				font-size: 10pt;
				font-style: normal;
				font-weight: normal;
				color: rgb(00,00,00);/*#000000;*/;
				background-color: rgb(255,255,255);
				-moz-border-radius: 15px;
				border-radius: 15px;
				-moz-box-shadow: 4px 4px 6px #888;
				-webkit-box-shadow: 4px 4px 6px #888;
				box-shadow: 4px 4px 6px #888;
}
.style10 {
				text-align: left;
}
.style11 {
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				font-weight: normal;
				color: #000000;
}
.style12 {
				font-family: Georgia, Times, serif;
				font-size: 10pt;
				font-style: normal;
				color: #000000;
}
.style13 {
				text-align: center;
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				color: #000000;
}
.style14 {
				font-family: Georgia, Times, serif;
				font-style: normal;
				font-weight: bold;
				color: #000000;
}
</style>

</head>

<body>
<div class="style9">
<div class="style8">
<span class="style3">Free-form Scanning of Non-planar Appearance with Neural Trace Photography<br />
</span>
<br />
<span class="style7">
    <a href="http://xiaohema98.com/">Xiaohe Ma</a>,
	  <a href="http://www.cocoakang.cn">Kaizhang Kang</a>, 
	  Ruisheng Zhu, 
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a> and
	  <a href="http://kunzhou.net/">Kun Zhou</a>
</span><br />
<span class="text"><br />
</span><span class="style11">ACM Trans. Graph. (Proc. SIGGRAPH 2021), 40, 4 (Aug. 2021), 124.<br />
Patent Pending.</span>
<br />
<em><br />
</em>
<br />
				<img alt="" src="teaser.jpg" width="700" class="papericon" /><br /><br />
<iframe class="papericon" width="700" height="394" src="https://www.youtube.com/embed/PnO7dZZDG2M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				<br />
				<br />
				<img src="paper01.jpg" width="114" class="papericon">
				<img src="paper02.jpg" width="114" class="papericon">
				<img src="paper03.jpg" width="114" class="papericon">
				<img src="paper04.jpg" width="114" class="papericon">
				<img src="paper05.jpg" width="114" class="papericon">
				<img src="paper06.jpg" width="114" class="papericon">
				<img src="paper07.jpg" width="114" class="papericon">
				<img src="paper08.jpg" width="114" class="papericon">
				<img src="paper09.jpg" width="114" class="papericon">
				<img src="paper10.jpg" width="114" class="papericon">
				<img src="paper11.jpg" width="114" class="papericon">
				<img src="paper12.jpg" width="114" class="papericon">
				<br />
				<br />
</div>
<hr />
</span>
<div class="style10">
				<span class="textsectionheader2"><br />
				Abstract</span><span class="text"><br />
				<br />
We propose neural trace photography, a novel framework to automatically learn high-quality scanning of non-planar, complex anisotropic appearance. Our key insight is that free-form appearance scanning can be cast as a geometry learning problem on unstructured point clouds, each of which represents an image measurement and the corresponding acquisition condition. Based on this connection, we carefully design a neural network, to jointly optimize the lighting conditions to be used in acquisition, as well as the spatially independent reconstruction of reflectance from corresponding measurements. Our framework is not tied to a specific setup, and can adapt to various factors in a data-driven manner. We demonstrate the effectiveness of our framework on a number of physical objects with a wide variation in appearance. The objects are captured with a light-weight mobile device, consisting of a single camera and an RGB LED array. We also generalize the framework to other common types of light sources, including a point, a linear and an area light.<br />
				</span><br />
				<hr /><br />
				<span class="textsectionheader2">Downloads
				<br />
				</span>
				<span class="style14">
				<br />
				</span><span class="style12">Paper <a href="scanner.pdf">[.PDF, 37.7MB]</a> <a href="https://doi.org/10.1145/3450626.3459679">[ACM DL (Open Access)]</a></span><br />
				<br />
				Supplemental Material <a href="supp.pdf">[.PDF]</a><br />
				<br />
				Bibtex <a href="scanner.bib">[.BIB]</a><br />
				<br />
				Video
				<a href="scanner.mp4">[.MP4, 59.6MB]</a>
				<a href="https://youtu.be/PnO7dZZDG2M" target=" _blank">[Youtube]</a>
				<a href="https://www.bilibili.com/video/BV1WV41147NA" target=" _blank">[Bilibili]</a>		<br />
				<br />
				Slides <a href="slides.pdf">[.PDF, 27.1MB]</a><br />
				<br />
				</span> 
				<hr />				
<br />
<div class="style13">
<a href="../../index.html">Back</a></div>
</div>
</body>

</html>
