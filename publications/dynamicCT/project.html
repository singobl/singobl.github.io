<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="Content-Language" content="zh-cn" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Differentiable Dynamic Visible-Light Tomography</title>
<link href="../../gly.css" rel="stylesheet" type="text/css">

<style type="text/css">
.style3 {
				font-family: Georgia, Times, serif;
				font-size: 16pt;
				font-style: normal;
				color: #000000;
}
.style7 {
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				color: #000000;
}
.style8 {
				text-align: center;
}
.style9 {
				/* fixed width */
	margin: 0 auto;
				padding: 30px;
				width: 720px;
				text-align: center;
				position: relative;
				font-family: Georgia, Times, serif;
				font-size: 10pt;
				font-style: normal;
				font-weight: normal;
				color: rgb(00,00,00);/*#000000;*/;
				background-color: rgb(255,255,255);
				-moz-border-radius: 15px;
				border-radius: 15px;
				-moz-box-shadow: 4px 4px 6px #888;
				-webkit-box-shadow: 4px 4px 6px #888;
				box-shadow: 4px 4px 6px #888;
}
.style10 {
				text-align: left;
}
.style11 {
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				font-weight: normal;
				color: #000000;
}
.style12 {
				font-family: Georgia, Times, serif;
				font-size: 10pt;
				font-style: normal;
				color: #000000;
}
.style13 {
				text-align: center;
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				color: #000000;
}
.style14 {
				font-family: Georgia, Times, serif;
				font-style: normal;
				font-weight: bold;
				color: #000000;
}
</style>

</head>

<body>
<div class="style9">
<div class="style8">
<span class="style3">Differentiable Dynamic Visible-Light Tomography<br />
</span>
<br />
<span class="style7">
    <a href="http://www.cocoakang.cn">Kaizhang Kang#</a>, 
    Zoubin Bi#, Xiang Feng, Yican Dong*, <a href="http://kunzhou.net/">Kun Zhou</a> and <a href="https://svbrdf.github.io/">Hongzhi Wu</a><br>
</span><br />
<span class="text"><br />
</span><span class="style11">ACM SIGGRPAH Asia 2023.<br />
Patent Pending.<br />
<br />
<br />
<iframe class="papericon" width="700" height="394" src="https://www.youtube.com/embed/96E-hF8Oqp4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br />
<br />
				<img src="paper01.jpg" width="115" class="papericon">
				<img src="paper02.jpg" width="115" class="papericon">
				<img src="paper03.jpg" width="115" class="papericon">
				<img src="paper04.jpg" width="115" class="papericon">
				<img src="paper05.jpg" width="115" class="papericon">
				<img src="paper06.jpg" width="115" class="papericon">
				<img src="paper07.jpg" width="115" class="papericon">
				<img src="paper08.jpg" width="115" class="papericon">
				<img src="paper09.jpg" width="115" class="papericon">
				<img src="paper10.jpg" width="115" class="papericon">
				<img src="paper11.jpg" width="115" class="papericon">
				<img src="paper12.jpg" width="115" class="papericon">
<br />
</span>
<div class="style10">
				<span class="textsectionheader2"><br />
				Abstract</span><span class="text"><br />
				<br />
We propose the first visible-light tomography system for real-time acquisition and reconstruction of general temporally-varying 3D phenomena. Using a single high-speed camera, a high-performance LED array and optical fibers with a total length of 5 km, we build a novel acquisition setup with no mechanical movements to simultaneously sample using 1,920 interleaved sources and detectors with a complete 360-degree coverage. Next, we introduce a novel differentiable framework to map both tomography acquisition and reconstruction to a carefully designed autoencoder. This allows the joint and automatic optimization of both processes in an end-to-end fashion, essentially learning to physically compress and computationally decompress the target information. Our framework can adapt to various factors, and trade between capture speed and reconstruction quality. We achieve an acquisition speed of up to 36.8 volumes per second at a spatial resolution of 32×128×128; each volume is captured with as few as 8 images. The effectiveness of the system is demonstrated on acquiring various dynamic scenes. Our results are also validated with the reconstructions computed from the measurements with one source on at a time, and compare favorably with state-of-the-art techniques.			
				<br />
				</span><br />
				<hr /><br />
				<span class="textsectionheader2">Downloads
				</span>
				<br />
				<br />
				<a href="dynamicCT.pdf">Paper [.PDF (48.6MB)]</a><br />
				<br />
				Video [<a href="https://youtu.be/96E-hF8Oqp4">Youtube</a>/<a href="https://www.bilibili.com/video/BV1YP411b7RA/">Bilibili</a>] <br />
				<br />
				Bibtex [.BIB] <br />
				<br />
				</span> 
				<hr />				
<br />
<br />
<div class="style13">
<a href="../../index.html">Back</a></div>
</div>
</body>

</html>
