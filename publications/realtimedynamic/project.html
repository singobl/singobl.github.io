<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="Content-Language" content="zh-cn" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Real-time Acquisition and Reconstruction of Dynamic Volumes with Neural Structured Illumination</title>
<link href="../../gly.css" rel="stylesheet" type="text/css">

<style type="text/css">
.style3 {
				font-family: Georgia, Times, serif;
				font-size: 16pt;
				font-style: normal;
				color: #000000;
}
.style7 {
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				color: #000000;
}
.style8 {
				text-align: center;
}
.style9 {
				/* fixed width */
	margin: 0 auto;
				padding: 30px;
				width: 720px;
				text-align: center;
				position: relative;
				font-family: Georgia, Times, serif;
				font-size: 10pt;
				font-style: normal;
				font-weight: normal;
				color: rgb(00,00,00);/*#000000;*/;
				background-color: rgb(255,255,255);
				-moz-border-radius: 15px;
				border-radius: 15px;
				-moz-box-shadow: 4px 4px 6px #888;
				-webkit-box-shadow: 4px 4px 6px #888;
				box-shadow: 4px 4px 6px #888;
}
.style10 {
				text-align: left;
}
.style11 {
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				font-weight: normal;
				color: #000000;
}
.style12 {
				font-family: Georgia, Times, serif;
				font-size: 10pt;
				font-style: normal;
				color: #000000;
}
.style13 {
				text-align: center;
				font-family: Georgia, Times, serif;
				font-size: 12pt;
				font-style: normal;
				color: #000000;
}
.style14 {
				font-family: Georgia, Times, serif;
				font-style: normal;
				font-weight: bold;
				color: #000000;
}
</style>

</head>

<body>
<div class="style9">
<div class="style8">
<span class="style3">Real-time Acquisition and Reconstruction of Dynamic Volumes with Neural Structured Illumination<br />
</span>
<br />
<span class="style7">
    <a href="https://zyx45889.github.io/">Yixin Zeng#</a>, 
    <a href="https://github.com/RupertPaoZ/">Zoubin Bi#</a>, Mingrui Yin*, Xiang Feng, <a href="http://kunzhou.net/">Kun Zhou</a> and <a href="https://svbrdf.github.io/">Hongzhi Wu</a><br>
</span><br />
<span class="text"><br />
</span><span class="style11">IEEE CVPR 2024.<br />
Patent Pending.<br />
<br />
<br />
<iframe class="papericon" width="700" height="394" src="https://www.youtube.com/embed/XoTYTGSueh4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br />
<br />
				<img src="paper01.jpg" width="115" class="papericon">
				<img src="paper02.jpg" width="115" class="papericon">
				<img src="paper03.jpg" width="115" class="papericon">
				<img src="paper04.jpg" width="115" class="papericon">
				<img src="paper05.jpg" width="115" class="papericon">
				<img src="paper06.jpg" width="115" class="papericon">
				<img src="paper07.jpg" width="115" class="papericon">
				<img src="paper08.jpg" width="115" class="papericon">
				<img src="paper09.jpg" width="115" class="papericon">
				<img src="paper10.jpg" width="115" class="papericon">
				<img src="paper11.jpg" width="115" class="papericon">
				<img src="paper12.jpg" width="115" class="papericon">
<br />
</span>
<div class="style10">
				<span class="textsectionheader2"><br />
				Abstract</span><span class="text"><br />
				<br />
We propose a novel framework for real-time acquisition and reconstruction of temporally-varying 3D phenomena with high quality. The core of our framework is a deep neural network, with an encoder that directly maps to the structured illumination during acquisition, a decoder that predicts a 1D density distribution from single-pixel measurements under the optimized lighting, and an aggregation module that combines the predicted densities for each camera into a single volume. It enables the automatic and joint optimization of physical acquisition and computational reconstruction, and is flexible to adapt to different hardware configurations. The effectiveness of our frame- work is demonstrated on a lightweight setup with an off- the-shelf projector and one or multiple cameras, achieving a performance of 40 volumes per second at a spatial resolution of 1283. We compare favorably with state-of-the-art techniques in real and synthetic experiments, and evaluate the impact of various factors over our pipeline.			
				<br />
				</span><br />
				<hr /><br />
				<span class="textsectionheader2">Downloads
				</span>
				<br />
				<br />
				<a href="realtimedynamic.pdf">Paper [.PDF (5.4MB)]</a><br />
				<br />
				Video [<a href="https://youtu.be/XoTYTGSueh4">Youtube</a>/<a href="https://www.bilibili.com/video/BV1P2421N7qR/">Bilibili</a>] <br />
				<br />
				Bibtex [.BIB] <br />
				<br />
				Code [Coming Soon...] <br />
				<br />
				</span> 
				<hr />				
<br />
<br />
<div class="style13">
<a href="../../index.html">Back</a></div>
</div>
</body>

</html>
